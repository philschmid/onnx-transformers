{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark wav2vec2 performance in onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install librosa for getting duration of audio files\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: onnxruntime-gpu 1.10.0\n",
      "Uninstalling onnxruntime-gpu-1.10.0:\n",
      "  Successfully uninstalled onnxruntime-gpu-1.10.0\n",
      "Collecting onnxruntime\n",
      "  Using cached onnxruntime-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime) (3.17.3)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9 in /home/ubuntu/.local/lib/python3.8/site-packages (from protobuf->onnxruntime) (1.15.0)\n",
      "Installing collected packages: onnxruntime\n",
      "Successfully installed onnxruntime-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall  onnxruntime-gpu -y\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model at: /home/ubuntu/onnx-transformers/speech/exports/wav2vec2-base-960h.onnx\n",
      "input_names:  ['input_values']\n",
      "current input shape {'input_values': torch.Size([1, 219040])}\n",
      "Using framework PyTorch: 1.10.0+cu102\n",
      "output_names:  ['logits']\n",
      "dynamic_axes:  {'input_values': {0: 'batch_size', 1: 'sequence'}, 'logits': {0: 'batch_size', 1: 'sequence'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:2359: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:553: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:590: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized model saved at: /home/ubuntu/onnx-transformers/speech/exports/wav2vec2-base-960h.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized model saved at: /home/ubuntu/onnx-transformers/speech/exports/wav2vec2-base-960h-q8.onnx\n",
      "outpus are different\n"
     ]
    }
   ],
   "source": [
    "# create onnx modelsf\n",
    "from convert_wav2vec2 import convert_wav2vec2_onnx\n",
    "\n",
    "!rm -rf ./exports\n",
    "\n",
    "model_id = \"facebook/wav2vec2-base-960h\"\n",
    "convert_wav2vec2_onnx(model_id=model_id, optimize=True, quantize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ONNX CPU Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np \n",
    "from transformers import Wav2Vec2Processor\n",
    "from convert_wav2vec2 import get_sample,get_inuputs_from_audio\n",
    "import time \n",
    "import librosa\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "# Set graph optimization level\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "model_path=\"exports/wav2vec2-base-960h-q8.onnx\"\n",
    "ort_session = ort.InferenceSession(model_path, sess_options)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def onnx_asr(path,sess,processor):\n",
    "    # get inputs\n",
    "    onnx_inputs = get_inuputs_from_audio(path=path, processor=processor, tensor_type=\"np\")\n",
    "    # run inference\n",
    "    st = time.time()\n",
    "    logits = sess.run(None, onnx_inputs.data)[0]\n",
    "    dur = time.time() - st\n",
    "    # decode\n",
    "    predicted_ids=np.argmax(logits, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription, dur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running onnx inference with facebook/wav2vec2-base-960h on CPU\n",
      "Duration of sample1.flac is 13.69s\n",
      "Prediction with ORT took 1.25s\n",
      "Meaining: 1 second audio takes 0.09 seconds to predict\n",
      "transcript: \n",
      "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKIN TO DAMP AUDIENCES IN GRATHTY SCHOOLROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SUN PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS\n"
     ]
    }
   ],
   "source": [
    "print(f\"running onnx inference with {model_id} on CPU\")\n",
    "sample = get_sample(1)\n",
    "print(f\"Duration of {sample} is {librosa.get_duration(filename=sample)}s\")\n",
    "\n",
    "trans , duration = onnx_asr(sample,ort_session,processor)\n",
    "print(f\"Prediction with ORT took {round(duration,2)}s\")\n",
    "print(f\"Meaining: 1 second audio takes {round(round(duration,2)/librosa.get_duration(filename=sample),2)} seconds to predict\")\n",
    "print(f\"transcript: \\n{trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pytorch CPU Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor,Wav2Vec2ForCTC\n",
    "from convert_wav2vec2 import get_sample,get_inuputs_from_audio\n",
    "import time \n",
    "import librosa\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "def pytorch_asr(path,model,processor):\n",
    "    # get inputs\n",
    "    inputs = get_inuputs_from_audio(path=path, processor=processor, tensor_type=\"pt\")\n",
    "    # run inference\n",
    "    st = time.time()\n",
    "    logits = model(**inputs)[0]\n",
    "    dur = time.time() - st\n",
    "    # decode\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription, dur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running pytorch inference with facebook/wav2vec2-base-960h on CPU\n",
      "Duration of sample1.flac is 13.69s\n",
      "Prediction with Pytorch took 1.3s\n",
      "Meaining: 1 second audio takes 0.09 seconds to predict\n",
      "transcript: \n",
      "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS\n"
     ]
    }
   ],
   "source": [
    "print(f\"running pytorch inference with {model_id} on CPU\")\n",
    "sample = get_sample(1)\n",
    "print(f\"Duration of {sample} is {librosa.get_duration(filename=sample)}s\")\n",
    "\n",
    "trans , duration = pytorch_asr(sample,model,processor)\n",
    "print(f\"Prediction with Pytorch took {round(duration,2)}s\")\n",
    "print(f\"Meaining: 1 second audio takes {round(round(duration,2)/librosa.get_duration(filename=sample),2)} seconds to predict\")\n",
    "print(f\"transcript: \\n{trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU test (Not tested yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\n",
      "Requirement already satisfied: onnxruntime-gpu in /home/ubuntu/.local/lib/python3.8/site-packages (1.10.0)\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime-gpu) (1.12)\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime-gpu) (3.17.3)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime-gpu) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9 in /home/ubuntu/.local/lib/python3.8/site-packages (from protobuf->onnxruntime-gpu) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall onnxruntime -y\n",
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onnx modelsf\n",
    "from convert_wav2vec2 import convert_wav2vec2_onnx\n",
    "\n",
    "!rm -rf exports\n",
    "model_id = \"facebook/wav2vec2-base-960h\"\n",
    "convert_wav2vec2_onnx(model_id=model_id, optimize=True, quantize=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ONNX GPU Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np \n",
    "from transformers import Wav2Vec2Processor\n",
    "from convert_wav2vec2 import get_sample,get_inuputs_from_audio\n",
    "import time \n",
    "import librosa\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "# Set graph optimization level\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "model_path=\"exports/wav2vec2-base-960h-opt.onnx\"\n",
    "ort_session = ort.InferenceSession(model_path, sess_options,providers=['CUDAExecutionProvider'])\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def onnx_asr(path,sess,processor):\n",
    "    # get inputs\n",
    "    onnx_inputs = get_inuputs_from_audio(path=path, processor=processor, tensor_type=\"np\")\n",
    "    # run inference\n",
    "    st = time.time()\n",
    "    logits = sess.run(None, onnx_inputs.data)[0]\n",
    "    dur = time.time() - st\n",
    "    # decode\n",
    "    predicted_ids=np.argmax(logits, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription, dur\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
