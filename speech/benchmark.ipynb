{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark wav2vec2 performance in onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install librosa for getting duration of audio files\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: onnxruntime-gpu 1.10.0\n",
      "Uninstalling onnxruntime-gpu-1.10.0:\n",
      "  Successfully uninstalled onnxruntime-gpu-1.10.0\n",
      "Collecting onnxruntime\n",
      "  Using cached onnxruntime-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime) (3.17.3)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9 in /home/ubuntu/.local/lib/python3.8/site-packages (from protobuf->onnxruntime) (1.15.0)\n",
      "Installing collected packages: onnxruntime\n",
      "Successfully installed onnxruntime-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall  onnxruntime-gpu -y\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model at: /home/ubuntu/onnx-transformers/speech/exports/wav2vec2-base-960h.onnx\n",
      "input_names:  ['input_values']\n",
      "current input shape {'input_values': torch.Size([1, 219040])}\n",
      "Using framework PyTorch: 1.10.0+cu102\n",
      "output_names:  ['logits']\n",
      "dynamic_axes:  {'input_values': {0: 'batch_size', 1: 'sequence'}, 'logits': {0: 'batch_size', 1: 'sequence'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:2359: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:604: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:641: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EmbedLayerNormalization': 0, 'Attention': 12, 'Gelu': 8, 'FastGelu': 0, 'BiasGelu': 12, 'LayerNormalization': 1, 'SkipLayerNormalization': 25}\n",
      "optimized model saved at: /home/ubuntu/onnx-transformers/speech/exports/wav2vec2-base-960h_self.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator BiasGelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized model saved at: /home/ubuntu/onnx-transformers/speech/exports/wav2vec2-base-960h-q8.onnx\n",
      "outpus are different\n"
     ]
    }
   ],
   "source": [
    "# create onnx modelsf\n",
    "from convert_wav2vec2 import convert_wav2vec2_onnx\n",
    "\n",
    "!rm -rf ./exports\n",
    "\n",
    "model_id = \"facebook/wav2vec2-base-960h\"\n",
    "convert_wav2vec2_onnx(model_id=model_id, optimize=True, quantize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ONNX CPU Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np \n",
    "from transformers import Wav2Vec2Processor\n",
    "from convert_wav2vec2 import get_sample,get_inuputs_from_audio\n",
    "import time \n",
    "import librosa\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "# Set graph optimization level\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "model_id = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "model_path=\"exports/wav2vec2-base-960h-q8.onnx\"\n",
    "ort_session = ort.InferenceSession(model_path, sess_options)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def onnx_asr(path,sess,processor):\n",
    "    # get inputs\n",
    "    onnx_inputs = get_inuputs_from_audio(path=path, processor=processor, tensor_type=\"np\")\n",
    "    # run inference\n",
    "    st = time.time()\n",
    "    logits = sess.run(None, onnx_inputs.data)[0]\n",
    "    dur = time.time() - st\n",
    "    # decode\n",
    "    predicted_ids=np.argmax(logits, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription, dur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running onnx inference with facebook/wav2vec2-base-960h on CPU\n",
      "Duration of yt.mp3 is 192.0s\n",
      "Prediction with ORT took 52.54s\n",
      "Meaining: 1 second audio takes 0.27 seconds to predict\n",
      "transcript: \n",
      "O EVERY ONE ON NAT HUD PACE  TO DAY ER IN A FINE TUN OVISION TRANSFORMER FOR IN AGE FLATATION ESN P  FOCH PUT TOGETHER OMAKE A SUPER EA  TRAIN OVISION TRANSFORMER OR ANYTHING YOUSN PICTES ON THE WAPASTEDLY ALL YEU TO DUSE THO FIN THE PLASES YOU LO OE IN AGE FOTS FARE FOR AND THAT WE USE AND INS SEARCHA YA  FLECT IN AGES OF THOSE  LA S  AT TO THAT IL TRAIN OF THISION TRANSFORMER A THE IMAGES WE CLC  WE SHOP O THE PHUTN PACE UP  SO ANY ONE AROUD THE WORLD A TRIU RH  TO GOSTARTED YOU GO THE HUNN PICH WE GON GETOD PIT OPEN IN COLAT TACTES  DAM AS A PRELIMINARY STEP WOLN STALL THE REPAR T ANS PWON STATS TAN PLY WER THREE TO DEFINE OUR SEARCH TURMS TAKE E A TE PASSES THAT WE MON FINTION AR MOL PREDICT WHAT' SAY WE WANT THA GADOG RE PA AC WE SIPLY RITE THE NAME TO THE H TE DOG BREES ONT TACT PLOTERS TE RUN SEL THE LOCKIN OR  NOW WE'LL RUN THEN EXCABLE SELLS TO DOWN LO INAGES OF THOSE TOKS WONT TO FINISHHES L PAR OUR TRAINING IN VALIDATION TATSES AS FIT FORH STAT SETS MAN VISUALINE ONXAMPLES OF THE IMAGES WE DOWN LO THE TE SATHER WO SINCE THE IMAGE SEARCH ITI ISN ERFIC AT THIS POINT YOU MAY NEE TO GO BACK ANWEET TO SURCH TURMS WNRPEA THE SELLS  FO  O THE INGES LO  FUT AR THESE O TI E ME  SO POMA FORK THE NEXT STEP IS TOPAR THE LABEL MAPPING FO MODEPOT BY ADDING LABEL TO IDEA AND IDY THE LABEL MAPPING ON TOR MOS COTIC WILL GET FRIENDLIER LABELS IN THE HUND PACE ANRENSE YAK  NEXTABLE TOFINE OU TOTINPOLATER FLASS THAT WILL PAREAMPLES IN T E BACTES PECTNY PON PACK INTOR MOGREAT NNOWTS IN O FLINES THAT THE A TO PEATUR STRACTER PRE TRAIN MODEL ON THE COL LATER AND PEPARE OUR FI FOR STATAL OADERS OR THE TRAINING AVALITY YU TH FIND TWUNIN SEL WERE ANY USE PIOR FLUTNING AYU WE SEE AS O FLASS FIRYS IN LIGTING THAT FIND TE TRAININ STE HE VALIDATION STEP AN TA TRAINING OUP  IN IS CASE ER USE ATEM WITH NO LEARNING  O GAY WORD I THE TRAIN I SHOWIN TE AE MINETR U  FO  WA AWANCE THAT STEN ITS THE MOMENT TRUE WPHATS Y THE WORK  HOUGH HERE WE SEE THE PDIC TO LABELS FORS THE ACTFULLABELS AND THI SI E U FINALLY WE E SHARE THIS FINDY MODEL ON HUN PASUP  NEED A HN PAS COUT SO T AVAN ALREAT  TE SON U   POPFON GAY YOU LEAN AC TO STELIN WI  RIT A SSTO HE DON'T HAVE ONE TO SAT ANY ON E ENAMEA WHAT EVER YO WANT AR THE MATSUROS PASF THAT HAS WRIGT ACSE WE PUSSHIN POUP THEO  TOPPY THAT DTOPEN AND PI BACKIN TH NO OK PLO NOW WE'S GIVEN AN ITS UP LOAD W WON TO FINISH OE SEE TO BEAU TIFUL PUT PACE MODER RE POS TRAINED FORTS NWE CN EDEN DRAG AN DROP INAGES O MODEL CAR INTO THIN FORS WHER JI TO SEENA TA RDI SELL THAT'S HORN PICTS EASIES WAY TO TRAIN OF DISN TRANSFORMER OR ANYTHING I PICES ON THE WE IF YOU HAVENT ALREADY E TO TRY E SUPERT EAS USE AND YOU DONY NASWRITIN OP HE SJONS TH YOU HELPEL WON'T YUS  NOCH OP AGAINS TAT LIE OWORE THE FEELING O VENTURESS MAY BE MAKE FRENCO SUBTRIVE U I YOU LOOK FOR MORE FRIENDS THAN JUST TRAVE U NO  YOU JOIN OUT THIS CORT SERVAANNACT WITH OVER TWO DOWS ON THER POTS FOR EN MACHINE R  THAT'S FFER TO DAY PACE LOCHING AND TOL THACTSTAN TE LACK\n"
     ]
    }
   ],
   "source": [
    "print(f\"running onnx inference with {model_id} on CPU\")\n",
    "sample = get_sample(1)\n",
    "sample = \"yt.mp3\"\n",
    "print(f\"Duration of {sample} is {librosa.get_duration(filename=sample)}s\")\n",
    "\n",
    "trans , duration = onnx_asr(\"yt.mp3\",ort_session,processor)\n",
    "print(f\"Prediction with ORT took {round(duration,2)}s\")\n",
    "print(f\"Meaining: 1 second audio takes {round(round(duration,2)/librosa.get_duration(filename=sample),2)} seconds to predict\")\n",
    "print(f\"transcript: \\n{trans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running onnx inference with facebook/wav2vec2-base-960h on CPU\n",
      "Duration of yt.mp3 is 192.0s\n",
      "Prediction with ORT took 52.71s\n",
      "Meaining: 1 second audio takes 0.27 seconds to predict\n",
      "transcript: \n",
      "AH HA EVERY ONE I'M NATE FROM HUGGIN FACE AND TO DAY WE'RE IN A FINE TUNE OF VISION TRANSFORMER FOR IMAGE CLASSIFICATION USING HUGGAN PIKS A PROJECT WE PUT TOGETHER TO MAKE IT SUPEREASY TO TRAN OF VISION TRANSFORMER FOR ANYTHING USING PICTURES ON THE WEB BASICALLY ALL YOU'VE TO DOSE TO FINE THE CLASSES YOUD LIKE TO BUILD AN IMAGE CLASS FIRE FOR AND THE WE'LL USE AN IMAGE SEARCH A P A TO COLLECT IMAGES OF THOSE DESIRED CLASSES AFTER THAT WE'LL TRAIN A VISION TRANSFORMER ON THE IMAGES WE COLLECT AND PUSH IT UP TO THE HUGAN FACE HUB SO ANY ONE AROUND THE WORLD CAN TRY OUT YOUR CREATION  TO GET STARTED YOUAN GO TO THE HUGAN PICKS REEPO AND GET HUB AND CLICK OPENING COALAB TO ACCES THE DEMO AS A PRELIMINARY STEP WE'LL INSTALL THE REQUIRED DO PENDANCIES ONCE THAT'S COMPLETE WE'RE FREE TO DEFINE OUR SEARCH TERMS AK A THE CLASSES THAT WE WANT TO FIND TUNE OUR MODEL TO PREDICT LATNCE SAY WE WAN TO MAKE A DOG BREED CLASS FIRE WE SIMPLY WITE THE NAMES OF THE DIFERENT DOG BREEDS IN THE TEXBOXES AND RUN THE SELL TO LOCK IN OUR CHOICES NOW WE'LL RUN THE EXCABL CELLS TO DOWNLOAD IMAGES OF THOSE DOGS ONCE TO FINISHES WE'L PREPARE OUR TRAINING AND VALIDATION DATOSETS AS PIETORCH DATOSETS AND VISUALIZE SOME EXAMPLES OF THE IMAGES WE DOWNLOADED TO SEHOW THEY LOOK SINCE THE IMAGE SEARCH A P ISN'T PERFECT AT THIS POINT YOU AY NEED TO GO BACK AND TWEET YOUR SEARCH TERMS AND REPEAT THE CELLS ABOVE UNTIL THE IMAGES LOOK GOOD BUT AH THESE LOOK OD TO ME SO WOMAFORMER THE NEXT STEP IS TO PREPARE THE LABEL MAPPING FOR THE MODEL CONFIG BY ADDING LABEL TO IDEA AND ID TO LABEL MAPPING ON TO OUR MODELSCONFIG WE'LL GET FRIENDLIER LABELS IN THE HUGGIN FACE INFERANCSA P A  NET WE'LLDEFINE OUR CUSTOM COLATER CLASS THAT WI'LL PREPARE EXAMPLES INTO BATCHES THAT CAN BE UNPACKED INTO OUR MODEL GREAT NOLITINISHLIZES THE V I TUFUTUR IXTRACTOR THE PRETRAINE MODEL AH THE COAL LATER AND PREPARE OUR PITORCH DATAL OADERS FOR THE TRAINING AND VALATION SETS TO T FIND TRUNING ITCELF WE'RE AN USE PITORCH LIGHTNING AN HERE WE SEE A SIMPLE CLASSFIRE USING LIGHTNING THAT TO FINDS THE TRAINING STEP THE VALIDATION STEP AND THE TRAINING OFT MIZER IN THIS CASE WHERE NYOU USE ADAM WITH NO LEARNING RASCETTULAR OKE WE'RE GOOD TO TRAIN I SHALD ONLY TAKE A INUTE OR TWO SO JUST WEWAY FOR THAT ONCE THAT'S DONE IT'S THE MOMEN OF TRUTH LET'S SEIVE AT WORK SO HERE WE SEE THE PREDICTED LABELS FOR S THE ACTUAL LABELS AND THIS LOOKS GOOD ENOUGH FOR ME FINALLY WE CAN SHARE THIS FIND TUNE MODEL ON HE HUGAN FACE HUB YOU'L NEED A HGN FASE ACCOUNT SO IF YOU HAVE AN ALREADY OU GO SIN UP FOR FREE TO AUTHUNT CATE YOU'LL NEED AN AXISS TOKEN WITH RIGHT AXCES SO YOUDON'T HAVE ONE JUST AT A NEW ONE YOU CAN NAME IT WHAT EVER YOU WANT AH BUT MASUROUS PASIFY THAT HAS RIGHT ACCESS WEN WE PUSHING UP TO THE HUB WE COPY THAT TOKEN AND PACE O BACK IN THE NOTE BOOK TO AUTHENTICATE NOW WE'S GIVE IN A MINUTE TO UPLOAD AND WHAT TOS FINISHED WE SE THIS BEAUTIFUL HUGAN FACE MODELRE POES CREATED FOR US AND WE CAN EVEN DRAG AND DROP THE IMAGES FROM THE MODEL CARD INTO THE INFERENTS WIDGET TO SEE AN EXAMPLE PREDICTION SO THAT'S HUGGAN PICKS THE EASIEST WAY TO TRAIN OF VISION TRANSFORMER FOR ANYTHING USING PICTUES ON THE WEB IF YOU HAE AN ALREADY GIVE IT TO TRY IT'S SUPERDEASY USE AND YOU DON'T EVEN HAVE TO WITE ANY CUB IF YOU OM TIS VIDYUA HELPFUL WON'T JUS  NODGE UP AGAINST THAT LIKE BUT E'RE FOR FEELING ADVENTURESS MAYBE VEN MAKE FRIENDS WITH T SUBSCRIBE BUTN IF YOU LOOKIN FOR MORE FRIENDS THAN JUST TH SUBSCRIBE BUTTON NO YU CAN DRIN OUR DISCORT SERVER TO CONNECT WITH OVER TWO THOUSAND OTHER FOLKS INTEREST EN MACHINE LEARNING THAT'S ALF FOR TO DAY THANKS FOR WATCHING AND UNTIL NEXT TIME BUT BY\n"
     ]
    }
   ],
   "source": [
    "print(f\"running onnx inference with {model_id} on CPU\")\n",
    "sample = get_sample(1)\n",
    "sample = \"yt.mp3\"\n",
    "print(f\"Duration of {sample} is {librosa.get_duration(filename=sample)}s\")\n",
    "\n",
    "trans , duration = onnx_asr(\"yt.mp3\",ort_session,processor)\n",
    "print(f\"Prediction with ORT took {round(duration,2)}s\")\n",
    "print(f\"Meaining: 1 second audio takes {round(round(duration,2)/librosa.get_duration(filename=sample),2)} seconds to predict\")\n",
    "print(f\"transcript: \\n{trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pytorch CPU Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor,Wav2Vec2ForCTC\n",
    "from convert_wav2vec2 import get_sample,get_inuputs_from_audio\n",
    "import time \n",
    "import librosa\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "def pytorch_asr(path,model,processor):\n",
    "    # get inputs\n",
    "    inputs = get_inuputs_from_audio(path=path, processor=processor, tensor_type=\"pt\")\n",
    "    # run inference\n",
    "    st = time.time()\n",
    "    logits = model(**inputs)[0]\n",
    "    dur = time.time() - st\n",
    "    # decode\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription, dur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running pytorch inference with facebook/wav2vec2-base-960h on CPU\n",
      "Duration of sample1.flac is 13.69s\n",
      "Prediction with Pytorch took 1.3s\n",
      "Meaining: 1 second audio takes 0.09 seconds to predict\n",
      "transcript: \n",
      "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS\n"
     ]
    }
   ],
   "source": [
    "print(f\"running pytorch inference with {model_id} on CPU\")\n",
    "sample = get_sample(1)\n",
    "print(f\"Duration of {sample} is {librosa.get_duration(filename=sample)}s\")\n",
    "\n",
    "trans , duration = pytorch_asr(sample,model,processor)\n",
    "print(f\"Prediction with Pytorch took {round(duration,2)}s\")\n",
    "print(f\"Meaining: 1 second audio takes {round(round(duration,2)/librosa.get_duration(filename=sample),2)} seconds to predict\")\n",
    "print(f\"transcript: \\n{trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU test (Not tested yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\n",
      "Requirement already satisfied: onnxruntime-gpu in /home/ubuntu/.local/lib/python3.8/site-packages (1.10.0)\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime-gpu) (1.12)\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime-gpu) (3.17.3)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from onnxruntime-gpu) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9 in /home/ubuntu/.local/lib/python3.8/site-packages (from protobuf->onnxruntime-gpu) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall onnxruntime -y\n",
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onnx modelsf\n",
    "from convert_wav2vec2 import convert_wav2vec2_onnx\n",
    "\n",
    "!rm -rf exports\n",
    "model_id = \"facebook/wav2vec2-base-960h\"\n",
    "convert_wav2vec2_onnx(model_id=model_id, optimize=True, quantize=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ONNX GPU Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np \n",
    "from transformers import Wav2Vec2Processor\n",
    "from convert_wav2vec2 import get_sample,get_inuputs_from_audio\n",
    "import time \n",
    "import librosa\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "# Set graph optimization level\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "model_path=\"exports/wav2vec2-base-960h-opt.onnx\"\n",
    "ort_session = ort.InferenceSession(model_path, sess_options,providers=['CUDAExecutionProvider'])\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def onnx_asr(path,sess,processor):\n",
    "    # get inputs\n",
    "    onnx_inputs = get_inuputs_from_audio(path=path, processor=processor, tensor_type=\"np\")\n",
    "    # run inference\n",
    "    st = time.time()\n",
    "    logits = sess.run(None, onnx_inputs.data)[0]\n",
    "    dur = time.time() - st\n",
    "    # decode\n",
    "    predicted_ids=np.argmax(logits, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription, dur\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
